;;;Rector;;;Emma;;;;
;;Feasible;Measurable;Relevant;Feasible;Measurable;Relevant;Overall;;
RQ-ID;research_question;/3;/3;/3;/3;/3;/3;/9;Rector's Comment;Emma's Comment
RQ-01;Can large language models improve full sentence transliteration accuracy when fine-tuned on simulated parallel data versus standalone non-parallel data?;;;;;;;;;
RQ-02;Does the use of contextual information in full sentence transliteration systems improve performance in South Asian languages compared to standalone transliteration systems?;;;;;;;;;
RQ-03;Can a supervised learning approach using a Transformer-based architecture improve the accuracy of event nominal detection in Mandarin Chinese compared to traditional machine learning methods?;;;;;;;;;
RQ-04;Can the application of Universal Grammar-inspired schema improve the inter-annotator agreement for identifying event nominals in Mandarin Chinese by 10% or more?;;;;;;;;;
RQ-05;Can hierarchical Bayesian modeling be used to quantify the magnitude of bias in word embeddings by analyzing the uncertainty of the model parameters and the embedded vectors?;;;;;;;;;
RQ-06;Can hierarchical Bayesian modeling effectively detect bias in word embeddings at different levels of granularity, and can it provide a more nuanced understanding of the bias landscape than single-number metrics?;;;;;;;;;
RQ-07;Can a Transformer-based model with a novel method of incorporating semantic similarity and word embeddings improve topic modeling accuracy by detecting uncommon and unseen words in large text corpora?;;;;;;;;;
RQ-08;Can the proposed method's evaluation metrics based on intruder words and similarity measures be used to effectively compare the performance of topic modeling and document clustering models?;;;;;;;;;
RQ-09;What are the strengths and weaknesses of similarity-based methods for evaluating the faithfulness of end-to-end neural NLP models?;;;;;;;;;
RQ-10;How do self-explanatory models contribute to achieving faithful explanations in NLP, and what are the limitations of these approaches?;;;;;;;;;
RQ-11;Can machine learning algorithms be used to improve the decipherment of Linear B script using a combination of palaeography and computational models, and what metrics would be most suitable for evaluating their effectiveness?;;;;;;;;;
RQ-12;Can the use of natural language processing techniques enable the automated recognition of symbols in the Archanes script, and what are the potential challenges in developing a reliable algorithm for this task?;;;;;;;;;
RQ-13;Can machine learning models be designed to accurately predict typological features in a way that is grounded in linguistic theory and meets the needs of both NLP practitioners and linguists specialized in typology and language documentation?;;;;;;;;;
RQ-14;Can the use of multimodal data, such as audio and visual recordings, improve the accuracy of typological feature prediction and increase alignment between linguists and NLP researchers?;;;;;;;;;
RQ-15;Is it possible to develop a systematic approach to prevent coding errors in NLP evaluation experiments and improve the accuracy of reported numerical results through better code development practices?;;;;;;;;;
RQ-16;Can standardized pre-registration of NLP evaluation experiments reduce the occurrence of flaws such as ad hoc exclusion of participants and responses?;;;;;;;;;
RQ-17;How can the current logic-based syntheses of hallucination and omission in data-text NLG be evaluated for their accuracy in capturing the nuances of NLG models, and what metrics can be used to measure their effectiveness in identifying these phenomena?;;;;;;;;;
RQ-18;Can the proposed logic-based synthesis of hallucination and omission in data-text NLG address the limitations of current thinking and provide a more comprehensive understanding of these complex phenomena in LLMs?;;;;;;;;;
RQ-19;Can machine learning models trained on datasets with low annotator agreement and high annotation error rates achieve better generalization performance than those trained on high-quality datasets with low annotation error rates?;;;;;;;;;
RQ-20;How do quality management practices in dataset creation affect the accuracy and reliability of natural language datasets used in scientific publications?;;;;;;;;;
RQ-21;What are the effects of word-level data augmentation using large language models on the accuracy of Chinese dialogue-level dependency parsing?;;;;;;;;;
RQ-22;Can LLM-based discourse-level data augmentation improve the parsing performance in dependencies among elementary discourse units?;;;;;;;;;
RQ-23;Can a sampling approach that mitigates bias from sampling improve the correlation between automated coherence metrics and human judgment for large corpora?;;;;;;;;;
RQ-24;Can a user study design that incorporates proxy tasks and analysis of human response at both the group and individual level provide a more comprehensive understanding of human perception of coherence in topic models?;;;;;;;;;
RQ-25;Can few-shot learning with open Large Language Models (LLMs) improve the performance of Relation Extraction (RE) models on natural products literature datasets, and what are the optimal parameters for fine-tuning these models on synthetic abstracts?;;;;;;;;;
RQ-26;;;;;;;;;;
RQ-27;;;;;;;;;;
RQ-28;;;;;;;;;;
RQ-29;;;;;;;;;;
RQ-30;;;;;;;;;;
RQ-31;;;;;;;;;;
RQ-32;;;;;;;;;;
RQ-33;;;;;;;;;;
RQ-34;;;;;;;;;;
RQ-35;;;;;;;;;;
RQ-36;;;;;;;;;;
RQ-37;;;;;;;;;;
RQ-38;;;;;;;;;;
RQ-39;;;;;;;;;;
RQ-40;;;;;;;;;;
RQ-41;;;;;;;;;;
RQ-42;;;;;;;;;;
RQ-43;;;;;;;;;;
RQ-44;;;;;;;;;;
RQ-45;;;;;;;;;;
RQ-46;;;;;;;;;;
RQ-47;;;;;;;;;;
RQ-48;;;;;;;;;;
RQ-49;;;;;;;;;;
RQ-50;;;;;;;;;;
;;;;;;;;;;