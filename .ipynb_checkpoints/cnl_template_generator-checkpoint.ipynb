{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c94254f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"CNL-template generator\n",
    "\n",
    "This script takes a CSV file that already contains *research questions* and\n",
    "converts each question into an *abstract Controlled-Natural-Language (CNL)\n",
    "pattern* by\n",
    "\n",
    "1. marking *Entity Chunks* (EC) and *Property Chunks* (PC) with numbered\n",
    "   placeholders (EC1, PC1, …) using SpaCy and a small set of linguistic rules;\n",
    "2. matching the resulting pattern against a library of CLaRO CNL templates;\n",
    "3. writing out a merged CSV that contains the original question, its pattern,\n",
    "   the matched template-ID (if any), and a flag indicating whether the pattern\n",
    "   exists in the library.\n",
    "\n",
    "Usage\n",
    "-----\n",
    "\n",
    "    python cnl_template_generator.py \\\n",
    "        --questions  research_questions.csv \\\n",
    "        --templates  CLaROv2.csv \\\n",
    "        --output     questions_with_templates.csv\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "# -----------------------------\n",
    "#  SpaCy initialisation\n",
    "# -----------------------------\n",
    "\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "except OSError:\n",
    "    raise RuntimeError(\n",
    "        \"SpaCy model 'en_core_web_sm' is not installed. \"\n",
    "        \"Run `python -m spacy download en_core_web_sm` first.\"\n",
    "    )\n",
    "\n",
    "# -----------------------------\n",
    "#  Chunk‑marking helpers\n",
    "# -----------------------------\n",
    "def _mark_chunk(cq: str, spans, chunktype: str, offset: int, counter: int):\n",
    "    \"\"\"Replace spans inside *cq* with the placeholder *chunktype{counter}*.\"\"\"\n",
    "    for (start, end) in spans:\n",
    "        cq = cq[: start - offset] + f\"{chunktype}{counter}\" + cq[end - offset :]\n",
    "        offset += (end - start) - len(chunktype) - 1  # minus one digit\n",
    "    return cq, offset\n",
    "\n",
    "\n",
    "def extract_EC_chunks(cq: str) -> str:\n",
    "    \"\"\"Replace entity chunks with numbered EC placeholders.\"\"\"\n",
    "\n",
    "    doc = nlp(cq)\n",
    "    rejecting_ec = {\n",
    "        \"what\",\"which\",\"when\",\"where\",\"who\",\"type\",\"types\",\"kinds\",\"kind\",\n",
    "        \"category\",\"categories\",\"difference\",\"differences\",\"extent\",\"i\",\"we\",\n",
    "        \"respect\",\"there\",\"not\",\"the main types\",\"the possible types\",\n",
    "        \"the types\",\"the difference\",\"the differences\",\"the main categories\",\n",
    "    }\n",
    "\n",
    "    counter, offset = 1, 0\n",
    "\n",
    "    # Special: How + ADJ + VERB\n",
    "    if len(doc) >= 3 and doc[0].text.lower() == \"how\" and doc[1].pos_ == \"ADJ\" and doc[2].pos_ == \"VERB\":\n",
    "        start, end = doc[1].idx, doc[1].idx + len(doc[1])\n",
    "        cq, offset = _mark_chunk(cq, [(start, end)], \"EC\", offset, counter)\n",
    "        counter += 1\n",
    "\n",
    "    for chunk in doc.noun_chunks:\n",
    "        start, end = chunk.start_char, chunk.end_char\n",
    "        text = cq[start - offset : end - offset]\n",
    "        if text.lower() in rejecting_ec:\n",
    "            continue\n",
    "        cq, offset = _mark_chunk(cq, [(start, end)], \"EC\", offset, counter)\n",
    "        counter += 1\n",
    "\n",
    "    # Ending adjective/verb\n",
    "    if len(doc) >= 3 and doc[-2].pos_ in {\"VERB\", \"ADJ\", \"ADV\"} and doc[-1].text == \"?\":\n",
    "        start, end = doc[-2].idx, doc[-2].idx + len(doc[-2])\n",
    "        cq, offset = _mark_chunk(cq, [(start, end)], \"EC\", offset, counter)\n",
    "\n",
    "    return cq\n",
    "\n",
    "\n",
    "def _is_auxiliary(token, chunk_token_ids):\n",
    "    return token.head.i in chunk_token_ids and token.dep_ == \"aux\" and token.i not in chunk_token_ids\n",
    "\n",
    "\n",
    "def _get_pc_span(group: str, doc):\n",
    "    id_tags = group.split(\",\")\n",
    "    ids = [int(it.split(\"::\")[0]) for it in id_tags]\n",
    "    aux = next((t for t in doc if _is_auxiliary(t, ids)), None)\n",
    "    return (doc[ids[0]].idx, doc[ids[-1]].idx + len(doc[ids[-1]]), aux)\n",
    "\n",
    "\n",
    "def _reject_subspans(spans):\n",
    "    filtered = []\n",
    "    for i, (s_beg, s_end, *_rest) in enumerate(spans):\n",
    "        if not any(i != j and s_beg >= o_beg and s_end <= o_end for j, (o_beg, o_end, *_) in enumerate(spans)):\n",
    "            filtered.append((s_beg, s_end))\n",
    "    return filtered\n",
    "\n",
    "\n",
    "def get_PCs_as_spans(cq: str):\n",
    "    doc = nlp(cq)\n",
    "    pos_text = \",\".join(f\"{i}::{t.pos_}\" for i, t in enumerate(doc))\n",
    "    regexes = [\n",
    "        r\"([0-9]+::(PART|VERB),?)*([0-9]+::VERB)\",\n",
    "        r\"([0-9]+::(PART|VERB),?)+([0-9]+::AD(J|V),)+([0-9]+::ADP)\",\n",
    "        r\"([0-9]+::(PART|VERB),?)+([0-9]+::ADP)\",\n",
    "    ]\n",
    "    spans = []\n",
    "    for rx in regexes:\n",
    "        for m in re.finditer(rx, pos_text):\n",
    "            spans.append(_get_pc_span(m.group(), doc))\n",
    "    return _reject_subspans(spans)\n",
    "\n",
    "\n",
    "def extract_PC_chunks(cq: str) -> str:\n",
    "    rejecting_pc = {\"is\",\"are\",\"was\",\"were\",\"do\",\"does\",\"did\",\"have\",\"had\",\"can\",\"could\"}\n",
    "    offset, counter = 0, 1\n",
    "    for begin, end in get_PCs_as_spans(cq):\n",
    "        if cq[begin - offset : end - offset].lower() in rejecting_pc:\n",
    "            continue\n",
    "        cq, offset = _mark_chunk(cq, [(begin, end)], \"PC\", offset, counter)\n",
    "        counter += 1\n",
    "    return cq\n",
    "\n",
    "\n",
    "def to_cnl_pattern(question: str) -> str:\n",
    "    pattern = extract_EC_chunks(question)\n",
    "    pattern = extract_PC_chunks(pattern)\n",
    "    return re.sub(r\"\\s+\", \" \", pattern).strip()\n",
    "\n",
    "\n",
    "def load_questions(path: Path, column: str = \"question\") -> pd.DataFrame:\n",
    "    df = pd.read_csv(path)\n",
    "    if column not in df.columns:\n",
    "        raise KeyError(f\"Column '{column}' not found in {path}.\")\n",
    "    return df[[column]].rename(columns={column: \"question\"})\n",
    "\n",
    "\n",
    "def load_templates(path: Path) -> pd.DataFrame:\n",
    "    return pd.read_csv(path, sep=\";\", header=None, names=[\"ID\", \"pattern\"])\n",
    "\n",
    "\n",
    "def attach_templates(df_q: pd.DataFrame, templates: pd.DataFrame) -> pd.DataFrame:\n",
    "    df_q[\"pattern\"] = df_q[\"question\"].apply(to_cnl_pattern)\n",
    "    return df_q.merge(templates, how=\"left\", on=\"pattern\", indicator=\"Exist\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"Generate CNL templates from research questions CSV.\")\n",
    "    parser.add_argument(\"--questions\", required=True, help=\"CSV file containing research questions (column: 'question').\")\n",
    "    parser.add_argument(\"--templates\", default=\"CLaROv2.csv\", help=\"CLaRO template CSV.\")\n",
    "    parser.add_argument(\"--output\", default=\"questions_with_templates.csv\", help=\"Output CSV path.\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    df_q = load_questions(Path(args.questions))\n",
    "    templates = load_templates(Path(args.templates))\n",
    "    merged = attach_templates(df_q, templates)\n",
    "    merged.to_csv(args.output, index=False)\n",
    "    print(f\"✅  Saved {len(merged)} rows → {args.output}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
