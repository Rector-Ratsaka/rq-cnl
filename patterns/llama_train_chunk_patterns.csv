EC1 chunks,EC1 frequency,EC2 chunks,EC2 frequency,EC3 chunks,EC3 frequency,EC4 chunks,EC4 frequency,EC5 chunks,EC5 frequency,PC1 chunks,PC1 frequency,PC2 chunks,PC2 frequency
the integration,36,the-art,30,high accuracy,24,accuracy,34,accuracy,34,ec1,137,measured by,46
a transformer-based architecture,35,high accuracy,25,the-art,24,performance,26,the implications,25,ec1 using,91,capture,41
the incorporation,34,the quality,23,accuracy,20,languages,24,high accuracy,20,used to improve,61,trained on,36
the application,32,better performance,17,performance,19,low-resource languages,21,low-resource languages,17,outperform,58,to improve,25
large language models,25,higher accuracy,17,low-resource languages,19,they,21,they,17,enhance,53,leveraging,23
word embeddings,25,the nuances,17,machine translation systems,16,high accuracy,20,processing time,16,trained on,45,evaluating,22
the proposed method,23,machine translation systems,16,languages,16,the key factors,18,performance,16,measured by,43,detecting,21
the proposed approach,22,the efficiency,13,the quality,14,the impact,17,this approach,16,ec1 with,35,to achieve,17
the combination,20,word embeddings,13,they,14,the overall performance,16,english,15,trained,35,contribute to,17
machine learning models,20,machine translation models,12,better performance,12,their performance,16,metrics,15,provide,33,identify,17
the proposed model,18,bert,12,english,12,the implications,16,the key factors,14,to improve,32,predict,16
the addition,18,named entity recognition,12,text,12,the-art,15,the quality,13,leveraging,30,relate to,16
the inclusion,17,conjunction,12,the development,11,this approach,13,the impact,12,identify,30,maintaining,15
the development,16,the effectiveness,11,named entity recognition,11,machine translation systems,11,the-art,12,capture,29,reducing,15
a combination,16,low-resource languages,10,machine translation,11,the quality,11,words,11,translating,27,achieving,15
high accuracy,15,speech,10,the efficiency,10,a combination,10,traditional methods,11,used to develop,25,ec8,15
the effectiveness,15,-,10,the nuances,10,machine translation,10,their performance,11,reduce,25,provide,14
machine translation systems,15,the development,9,their performance,9,english,10,languages,10,detecting,25,used to improve,14
machine learning algorithms,14,the detection,9,the number,9,text,9,the effectiveness,10,utilizing,25,translating,14
a deep learning-based approach,14,text,9,speech,8,-,9,domain,10,enable,24,evaluate,14
language models,13,a combination,9,words,7,entities,9,precision,10,reducing,23,detect,14
sequence,13,fine-tuning,8,the robustness,7,sentences,8,models,9,evaluating,22,enhance,14
transformer-based models,11,language models,8,domain,7,low-resource language pairs,8,the model,9,used to evaluate,22,requiring,12
the proposed dataset,11,languages,8,bleu scores,7,machine translation tasks,8,machine translation systems,8,evaluated using,22,analyzing,12
the proposed system,10,machine learning models,8,the overall performance,7,speech,8,tasks,8,annotating,22,applied to,12
llms,10,performance,7,word embeddings,7,machine translation models,7,the development,8,learning,21,including,12
the proposed corpus,10,machine learning algorithms,7,neural machine translation systems,7,processing time,7,the effect,8,contribute to,21,outperform,11
a pre-trained language model,9,the translation quality,7,higher accuracy,7,the processing time,7,evaluation metrics,8,predicting,20,named,11
deep learning models,9,a transformer-based architecture,7,entities,7,the efficiency,7,linguistic features,8,improved,19,handling,11
the introduction,8,algorithm,7,the effectiveness,6,better performance,7,context,6,combining,19,classify,11
neural machine translation systems,8,domain,7,translations,6,llms,7,the overall performance,6,based,18,incorporates,11
the proposed framework,8,neural machine translation models,6,german,6,the model's performance,7,the limitations,6,including,17,generating,10
can the use,8,the processing time,6,a machine learning model,6,the challenges,7,-,6,ec1 for,17,reduce,10
machine translation models,8,accuracy,6,sentiment analysis,6,the nuances,7,a model,6,predict,17,evaluated by,10
the proposed methodology,8,back-translation,6,sequence,6,metrics,7,the number,6,used to train,16,evaluated using,10
a machine learning model,8,the number,6,entity,5,bert,7,bleu score,6,learn,15,handle,9
a deep learning model,8,pre-trained language models,6,sentences,5,the need,6,questions,5,develop,15,capturing,9
transformer-based language models,7,machine translation,6,french,5,the evaluation,6,existing methods,5,handling,15,to predict,9
multilingual models,7,entities,5,a large corpus,5,nlp tasks,6,limited training data,5,generating,15,improved by,8
pre-trained language models,7,english,5,machine translation models,5,a way,6,translation accuracy,5,named,14,utilizing,8
neural networks,7,llms,5,better results,5,a model,6,ability,5,improved by,14,extract,8
a machine learning-based approach,7,the transformer model,5,information,5,the effect,6,the baseline model,5,extracting,14,combining,8
end,7,comparable performance,5,training,5,sentiment analysis,6,translations,5,capturing,14,answering,8
knowledge distillation,6,a machine learning model,5,different languages,4,the relationship,6,the efficiency,5,detect,14,to generate,7
linguistic features,6,significant improvements,5,comparable performance,4,the size,6,the model's ability,5,design,14,combined with,7
the characteristics,6,the robustness,5,precision,4,these models,6,a dataset,5,ec1 utilizing,13,increasing,7
the-art,6,large language models,5,pre-trained language models,4,the f1-score,6,tweets,5,used to identify,13,learning,7
the implementation,6,the diversity,5,language models,4,improved performance,5,the reduction,5,applied to improve,13,used to develop,7
bert-based models,6,word,5,tweets,4,social media,5,improved performance,5,parsing,12,used to identify,6
the potential,6,text classification tasks,4,the correlation,4,multiple languages,5,wmt,5,ec8,12,creating,6
the number,5,the translation,4,the translation accuracy,4,the number,5,the f1-score,5,proposed,12,can ec,6
methods,5,better results,4,the representation,4,bleu scores,5,the percentage,5,used to predict,11,lead to,6
nmt models,5,comparable or better performance,4,the classification,4,a dataset,5,sentiment analysis,4,training,11,translate,6
the performance metrics,5,knowledge,4,translation accuracy,4,them,5,machine translation tasks,4,achieving,11,linking,6
can recurrent neural networks,5,comparable results,4,back-translation,4,the effectiveness,5,training,4,pairs,11,extracting,6
the evaluation,5,named entity recognition models,4,a combination,4,language models,5,the processing time,4,utilize,10,to enhance,6
speech,5,texts,4,dependency parsing,4,traditional methods,5,translation quality,4,lead to,9,training,6
the proposed methods,5,neural networks,4,the processing time,4,comparable performance,5,a combination,4,to achieve,9,predicting,6
the proposed model's ability,5,linguistic features,4,texts,4,precision,5,effectiveness,4,supervised,9,demonstrated by,6
a language model,5,the evaluation,4,tasks,4,words,5,errors,4,trained to improve,9,used to evaluate,6
a dataset,5,knowledge distillation,4,their ability,4,bleu score,5,machine translation,4,creating,9,parsing,5
a deep neural network,4,a pre-trained language model,4,fine-tuning,4,large language models,5,annotations,4,designed,9,enable,5
adversarial training,4,other languages,4,sentiment,4,text classification tasks,5,the model's performance,4,ec1 in,8,relying on,5
back-translation,4,a transformer architecture,4,machine learning algorithms,4,features,5,social media posts,4,providing,8,assessing,5
networks,4,transformers,4,conjunction,4,a high degree,5,machine translation models,4,existing,8,building,5
pre-trained models,4,features,4,context,4,machine learning models,4,higher accuracy,4,to generate,8,indicated by,5
multilingual embeddings,4,sentences,4,large language models,4,the addition,4,their ability,4,trained to achieve,8,understanding,5
the proposed algorithm,4,machine translation tasks,4,lexical resources,4,robustness,4,the precision,4,evaluated,8,tuned on,5
transformer models,4,the model,4,the implications,4,the bleu score,4,this improvement,4,tuning,8,extracted from,5
the design,4,byte pair encoding,4,the generation,4,fine-tuning,4,data,4,embedding,8,set,5
the conversion,4,the complexity,4,questions,4,better results,4,comparison,4,used to generate,8,annotating,5
the choice,4,sentiment analysis,4,japanese,4,limited training data,4,text,4,handle,8,to identify,5
a neural network architecture,4,approach,4,coreference resolution,4,techniques,4,these models,4,linking,7,learn,5
domain,4,a reliable evaluation metric,3,machine learning models,4,entity recognition,4,a way,4,used in,7,measure,5
training,4,the precision,3,features,4,errors,4,synthetic data,3,neural,7,providing,5
ensemble methods,4,the consistency,3,-,4,higher accuracy,4,universal dependencies,3,developed,7,generate,5
a model,4,algorithms,3,nlp models,4,language,4,natural language processing tasks,3,maintaining,7,improved,5
a transformer-based approach,4,the time complexity,3,the translation performance,3,texts,4,results,3,set,7,to detect,5
a pre-trained model,4,a baseline system,3,effectiveness,3,the development,4,the data,3,evaluate,7,based,5
a method,4,the correlation,3,metrics,3,tasks,4,downstream tasks,3,to enhance,7,adapted for,4
metrics,4,a significant improvement,3,sentiment analysis tasks,3,the correlation,4,the translation accuracy,3,requiring,6,leverage,4
a multi-task learning approach,4,dependency parsing,3,a multilingual model,3,human language,4,better performance,3,used to create,6,to capture,4
the key differences,4,the impact,3,documents,3,the model's ability,4,classification,3,used to enhance,6,achieves,4
transfer learning,4,relation extraction,3,linguistic features,3,evaluation metrics,4,literary texts,3,ec1 on,6,created using,4
a neural network model,4,consistent performance,3,social media,3,machine translation outputs,4,human judgments,3,pretrained,6,leading to,4
a convolutional neural network,4,dependency trees,3,the identification,3,other languages,4,user satisfaction,3,estimate,6,incorporate,4
can the application,3,translations,3,neural machine translation models,3,time,4,the evaluation metric,3,require,6,leverages,4
deep learning techniques,3,model,3,the evaluation,3,the benefits,4,the constraints,3,aligning,6,to reduce,4
the proposed ontology,3,distant supervision,3,data,3,the results,4,social media platforms,3,pretraining,6,reading,4
a transformer model,3,contextual information,3,human judgements,3,domain,4,a focus,3,answering,6,be improved,4
the fine-tuning,3,errors,3,the training process,3,high precision,4,language models,3,generate,6,trained,4
a machine translation system,3,the coverage,3,visual information,3,news articles,4,word embeddings,3,increasing,6,used to measure,4
a data-driven approach,3,utterances,3,back translation,3,the detection,4,the overall quality,3,uses,6,used to assess,4
the creation,3,sentence embeddings,3,the translation quality,3,a focus,4,50%,3,influence,6,learning from,4
a set,3,transformer models,3,a significant impact,3,tweets,4,users,3,trained to predict,6,model,4
synthetic data,3,multilingual models,3,semantic similarity,3,neural machine translation,3,the key features,3,to detect,6,measuring,4
the presence,3,bert embeddings,3,vocabulary,3,pre-trained language models,3,automatic metrics,3,classify,6,enabling,4
different types,3,human judgments,3,the proposed system,3,the creation,3,features,3,to design,6,combines,4
universal dependencies,3,natural language processing techniques,3,attention,3,the distribution,3,german,3,generated by,5,utilize,4
a semi-supervised learning approach,3,natural language processing models,3,neural networks,3,semantics,3,transformer models,3,ec1 leveraging,5,ec1 traine,4
a supervised approach,3,nlp models,3,other languages,3,sentence embeddings,3,the current state,3,measure,5,pairs,4
topic models,3,training,3,abusive language,3,syntactic correctness,3,topics,3,shared,5,quantify,4
lstm,3,transfer learning,3,data augmentation,3,back-translation,3,fine-tuning,3,developed to improve,5,embedding,4
data augmentation methods,3,words,3,the bleu score,3,task,3,language resources,3,extract,5,analyze,4
nmt systems,3,data augmentation,3,monolingual data,3,the model,3,corpora,3,to identify,5,recognize,4
the relationship,3,precision,3,-grams,3,the translation,3,the key challenges,3,developed using,5,uses,4
neural machine translation models,3,tweets,3,part,3,fake news,3,vocabulary,3,used as,5,ensuring,4
cross-lingual word embeddings,3,the accessibility,3,contextual information,3,word sense disambiguation,3,bleu,3,classifying,5,existing,4
a neural model,3,relations,3,competitive results,3,sentiment,3,the potential benefits,3,improved to achieve,5,derived from,3
a supervised classification model,3,word2vec,3,a dataset,3,their ability,3,different language pairs,3,modeling,5,can ec1,3
the linguistic features,3,information,3,models,3,additional training data,3,gender,3,demonstrated by,5,used for,3
the proposed annotation scheme,3,neural machine translation,3,bulgarian,3,different languages,3,their success,3,ec1 of,5,correcting,3
the likelihood,3,their performance,3,bert,3,tools,3,a corpus,3,to predict,5,trained with,3
the size,3,sequence,3,machine translation outputs,3,images,3,the benefits,3,evaluated by,5,determining,3
embeddings,3,pre-trained models,3,the conll 2018 shared task,3,the incorporation,3,better results,3,applied to,5,can ec1 tr,3
the proposed taxonomy,3,semantic parsing,3,translation,3,systems,3,a specific domain,3,reflect,4,to estimate,3
machine translation,3,editing,3,low-resource language pairs,3,topic modeling,3,knowledge bases,3,to create,4,vary across,3
the removal,3,reinforcement learning,3,the ability,3,analysis,3,nmt models,3,developed for,4,improved to achieve,3
the,3,models,3,coherence,3,transformer-based architectures,3,sentences,3,unsupervised,4,increase,3
data,3,medical terminology,3,the corpus,3,answers,3,techniques,3,to capture,4,features,3
words,3,images,3,the impact,3,the inclusion,3,methods,3,acquire,4,enhancing,3
named entity recognition,3,machine translation evaluation metrics,2,hate speech,3,reliability,3,model performance,3,trained with,4,of ec1,3
bleu scores,2,the strengths,2,nmt systems,3,comparable results,3,the training data,3,perform in,4,adapting to,3
a large-scale dataset,2,typological features,2,poetry,3,translations,3,the improvement,3,combined with,4,interpret,3
the limitations,2,recall,2,multilingual machine translation,3,the limitations,3,a high degree,3,map,4,to accommodate,3
the automatic metrics,2,word sense disambiguation tasks,2,ability,3,downstream tasks,3,limitations,3,to learn,4,to ec8,3
these models,2,the baseline system,2,a transformer-based architecture,3,effectiveness,3,multiple languages,3,containing,4,influencing,3
the methods,2,the generalization,2,domains,3,the key characteristics,3,task,3,incorporates,4,contributing to,3
sub-word embeddings,2,transliteration,2,target languages,3,the precision,3,the characteristics,3,vary,4,estimating,3
the transformation,2,the yoruba language,2,user satisfaction,2,word embeddings,3,images,3,to ec8,4,used in,3
slide,2,accessibility,2,bleu score,2,the translation quality,3,relatedness,3,identified,4,supervised,3
deep learning,2,the stability,2,the lack,2,comprehension,3,the complexity,3,reading,4,an ec1,3
can neural language models,2,text data,2,the inter-annotator agreement,2,uncertainty,3,the choice,3,increase,4,to extract,3
the factored transformer architecture,2,propositional idea density,2,consistency,2,the system,3,these methods,3,to extract,4,setting,3
syntactic information,2,comment-level data,2,neural language models,2,this improvement,3,a large corpus,3,used to achieve,4,preserving,3
bilingual translation systems,2,persian text,2,the alignment,2,high inter-annotator agreement,3,entity disambiguation tasks,2,influencing,4,aligned,3
the proposed algorithms,2,multilingual translation systems,2,ccg,2,neural machine translation models,3,limited annotated data,2,recurrent,4,achieved by,3
the proposed machine translation systems,2,the baseline model,2,the task,2,stance detection,3,different languages,2,derived from,4,to evaluate,3
morphological analysis,2,lstm hybrid neural network,2,a small dataset,2,text data,3,compliance,2,adding,4,yield,3
a framework,2,data selection,2,recurrent neural networks,2,patterns,3,word sense disambiguation tasks,2,annotate,4,have for,3
a deep cnn,2,large volumes,2,false information,2,semantic similarity,3,the conll 2017 ud shared task,2,analyzing,4,improved through,3
the performance differences,2,the dynamics,2,the reliability,2,varying levels,3,language pairs,2,produce,4,tuning,3
the proposed neural network architecture,2,the benefits,2,strong baselines,2,end,3,real-time,2,recognize,4,encoding,3
regime-specific surprisal estimates,2,statistical models,2,the comparability,2,french,3,the dataset,2,used to analyze,4,require,3
the proposed application,2,biomedical translation tasks,2,translation performance,2,specific aspects,3,concepts,2,learned by,3,captures,3
contextual information,2,different architectures,2,the training data,2,sequence,3,monolingual models,2,ec1 to,3,classifying,3
the gpt-4 model,2,the alice datasets,2,correlation,2,cases,3,treebanks,2,used for,3,to measure,3
data filtering,2,neural machine translation systems,2,fake news,2,the potential benefits,3,the system,2,differ from,3,optimizing,3
a lexicon-based approach,2,universal dependencies,2,multilingual machine translation models,2,consistency,3,word sense disambiguation systems,2,improve parsing,3,mitigate,3
the model's ability,2,the bleu scores,2,limited training data,2,traditional approaches,2,significant improvements,2,graph,3,applied to improve,3
a context-aware neural machine translation model,2,the mbart model,2,the relationship,2,human evaluation,2,multiple domains,2,estimating,3,processing,3
the approach,2,translation quality,2,multilingual translation systems,2,this task,2,the challenges,2,selecting,3,includes,3
accuracy,2,nmt systems,2,automatic evaluation metrics,2,language pairs,2,the ranking,2,resolving,3,algorithms,3
the volctrans system,2,word segmentation,2,data augmentation techniques,2,both languages,2,user engagement,2,quantified,3,evaluated,3
a nondeterministic stack rnn,2,correlation,2,african languages,2,the difficulty,2,embeddings,2,used to investigate,3,extending,3
the bleu scores,2,the distribution,2,significant performance gains,2,a significant factor,2,low-resource language pairs,2,ensembling,3,does ec1,2
machine learning-based approaches,2,the dataset,2,noisy data,2,reproducibility,2,reliability,2,personalizing,3,ec1 use,2
residual adapters,2,rnn language models,2,the detection,2,romance languages,2,overall performance,2,understanding,3,ec2 base,2
the proposed transformer-based architecture,2,the integration,2,multilingual language models,2,a supervised learning approach,2,machine learning techniques,2,improved to generate,3,shared,2
speech recognition systems,2,the system's performance,2,different domains,2,10%,2,these representations,2,adapted,3,apply,2
the quality,2,lexicon-free annotation,2,interactive agents,2,test sets,2,speech,2,recognizing,3,minimizing,2
a recurrent neural network,2,related languages,2,post-editing techniques,2,their accuracy,2,the models,2,ec1 embedding,3,support,2
the most accurate method,2,the attention mechanism,2,reasoning,2,the lack,2,their accuracy,2,integrating,3,to analyze,2
the proposed architecture,2,embeddings,2,catalan,2,word sense disambiguation tasks,2,the task,2,incorporate,3,relying solely on,2
dynamic fusion models,2,a language model,2,systems,2,hit-scir,2,the analysis,2,rely on,3,reveal,2
the inherent dependency displacement distribution,2,globalphone data,2,large volumes,2,a feasible approach,2,quality,2,to train,3,retrieving,2
different evaluation metrics,2,stocktwits data,2,morphologically rich languages,2,distribution,2,bert,2,to recognize,3,selecting,2
positional encoding,2,the coherence,2,real-time,2,the wmt22 shared task,2,the baseline system,2,predicted using,3,improve with,2
the potential impact,2,hate speech,2,multilingual corpora,2,dependency parsing,2,the recall,2,adapted to incorporate,3,used to achieve,2
webcrawl african corpora,2,cross-lingual word embeddings,2,named entities recognition,2,graphs,2,additional training data,2,tuned on,3,n ec4 compa,2
a synthetic corpus,2,distribution,2,the behavior,2,competitive performance,2,f1 score,2,learned,3,to handle,2
a hierarchical topic modeling approach,2,a transition-based algorithm,2,comparable corpora,2,idioms,2,natural language,2,adapting,3,proposed,2
position-based attention,2,transformer-based architectures,2,self-attention mechanism,2,natural language processing tasks,2,the news,2,improved to increase,3,test,2
pnns,2,the interpretation,2,parallel computation,2,historical texts,2,the complex relationships,2,to leverage,3,posed by,2
the training,2,challenge sets,2,t5,2,target languages,2,these changes,2,used to extract,3,replicated,2
a bert-based stance classifier,2,a dependency parser,2,semantic roles,2,a significant improvement,2,f1-score,2,ec1 to learn,3,improved to reduce,2
direct assessment,2,tokenization,2,the coherence,2,the evolution,2,data augmentation,2,relate to,3,utilizes,2
machine learning,2,mwes,2,semantic relation extraction,2,gpu,2,elmo,2,adapted to handle,3,transcribe,2
comboner,2,humans,2,unsupervised methods,2,russian language,2,a high proportion,2,trained to detect,3,used as,2
the system,2,conditional random fields,2,their elements,2,other approaches,2,the coverage,2,used to model,3,to recognize,2
a computational model,2,morphological features,2,thousands,2,the accessibility,2,the incorporation,2,developing,3,to ensure,2
the transformer model,2,urban dictionary,2,machine translation tasks,2,relation extraction models,2,the potential applications,2,varying,3,align,2
deep learning algorithms,2,a supervised classification model,2,zero pronouns,2,summaries,2,spanish,2,tuned with,3,to assess,2
this method,2,other domains,2,universal dependencies,2,user satisfaction,2,the increase,2,optimizing,3,assigning,2
the model,2,additional data,2,a language model,2,user engagement,2,text data,2,designing,3,constructing,2
the ability,2,simpler pre-trained models,2,re,2,both directions,2,the conditions,2,probing,3,n ec1 trai,2
a bert model,2,corpora,2,the construction,2,code-mixed text,2,repetition,2,architectures,3,ranking,2
classifiers,2,ensemble methods,2,queries,2,textual similarity tasks,2,social media,2,constructing,3,n ec1 wi,2
tupa,2,nmt models,2,new words,2,different types,2,different datasets,2,to measure,3,en compared,2
humans,2,the limitations,2,the data,2,90%,2,an accuracy,2,related to,3,optimized through,2
neural language models,2,child-directed speech,2,the issue,2,natural language text,2,this task,2,contributing to,3,developing,2
a significant reduction,2,attention,2,the annotation,2,interpretability,2,these approaches,2,sequence,3,trained to achieve,2
reinforcement learning,2,machine learning techniques,2,parsing models,2,the complexity,2,the results,2,representing,3,represent,2
the level,2,deep learning techniques,2,phonemes,2,financial texts,2,biomedical texts,2,used to measure,3,developed,2
the gdpr,2,linguistic properties,2,a set,2,domains,2,these corpora,2,improved through,3,dealing with,2
a non-autoregressive parser,2,other nlp tasks,2,transformer-based models,2,changes,2,mqm,2,decoding,3,ec1 be impr,2
a transformer-based language model,2,a higher accuracy,2,comet,2,a multilingual model,2,other approaches,2,measured,3,produce,2
eeg signals,2,they,2,the intelligibility,2,a significant impact,2,the trade-off,2,extracted from,3,enables,2
lda,2,different languages,2,methods,2,competitive results,2,the creation,2,comparing,3,to distinguish between,2
semi-supervised learning,2,portuguese,2,time,2,the performance metrics,2,training data,2,use,3,set for,2
pre-trained bert models,2,the transformer architecture,2,question,2,training,2,the resources,2,applying,3,reflect,2
lstm networks,2,domain knowledge,2,wmt20,2,f1 scores,2,the relationships,2,generative,3,used to enhance,2
a transformer architecture,2,a benchmark,2,entity recognition,2,universal dependencies,2,nmt systems,2,address,3,referring,2
word2vec,2,dependency,2,text anomalies,2,existing approaches,2,the optimal hyperparameters,2,features,3,generalize,2
the proposed ensemble approach,2,multilingual data,2,a way,2,dialogue systems,2,the input,2,crowdsourced,3,to assign,2
transfer,2,progressive learning,2,morphology,2,embeddings,2,these differences,2,reconstructing,3,address,2
transformer-based architectures,2,multilingual datasets,2,displacement calculus,2,the extraction,2,these metrics,2,algorithms,3,use,2
lit methods,2,figurative language indicators,2,past years' wmt competitions,2,teacher-student distillation,2,specific metrics,2,assessed using,3,quantified using,2
mbr,2,participants,2,overall performance,2,social media data,2,morphological segmentation,2,masked,3,benchmarks,2
lexical masks,2,input,2,the generalizability,2,the usability,2,linear logic derivations,2,applied,3,infer,2
the proposed mechanism,2,the identification,2,natural language processing tasks,2,these factors,2,error analysis,2,correlate with,2,used to train,2
multilingual bert,2,researchers,2,dialogue systems,2,under-resourced languages,2,sequence,2,preserve,2,to explain,2
transformers,2,topics,2,the potsdam commentary corpus,2,the optimal approach,2,efficiency,2,occurred,2,distinguish between,2
a semi-supervised approach,2,agreement,2,at least 90%,2,a more comprehensive understanding,2,analysis,2,differ between,2,trained to predict,2
the proposed joint learning method,2,the processing,2,multilingual machine translation systems,2,the target language,2,scenarios,2,using out,2,surpass,2
the proposed iso 24617-2 dialogue act annotation standard,2,the insertion transformer,2,the overall quality,2,human ratings,2,this ability,2,ec1 provided by,2,translating from,2
crfs,2,the complexities,2,insights,2,coverage,2,these systems,2,assessing,2,correlate with,2
pre-trained multilingual models,2,transformer architecture,2,quality estimation models,2,the fine-tuning phase,2,factors,2,used by,2,evaluated in,2
machine translation techniques,2,the open multilingual wordnet,2,resources,2,fake news detection,2,entity recognition,2,associated with,2,match,2
multilingual training,2,the graph-based parser,2,weak supervision,2,coherence,2,natural language understanding,2,cleaning,2,train,2
our proposed method,2,document classification,2,supervised models,2,the stability,2,the relationship,2,applied to support,2,c1 be improv,2
the open learner model,2,user satisfaction,2,domain-specific terminologies,2,the representation,2,udpipe,2,represent,2,required for,2
a multilingual bert model,2,competitive performance,2,fantasy literature,2,the similarity,2,neural machine translation systems,2,model,2,claim,2
seq2seq models,2,stress,2,dialects,2,ptg,2,the correlation,2,trained to learn,2,meaning,2
the annotation process,2,traditional machine learning methods,2,raw text,2,other domains,2,specific aspects,2,adjusting,2,compare,2
a rule-based approach,2,filtering,2,relevant information,2,word similarity,2,at least 90%,2,offer,2,measured in,2
a supervised classifier,2,text simplification,2,the training time,2,datasets,2,similarity,2,reveal,2,used to classify,2
the annotation scheme,2,traditional metrics,2,transformer-based architectures,2,claim verification,2,the bleu score,2,required for,2,extracted,2
automatic metrics,2,stories,2,a resource-rich language,2,bleu,2,machine learning algorithms,2,ec1 combining,2,an ec1 tra,2
kb-bert,2,word representation learning,2,grammatical error correction,2,biomedical text,2,them,2,bridging,2,to learn,2
the adoption,2,the prediction,2,training data,2,real-time applications,2,named entities,2,mitigate,2,removing,2
the pretraining,2,negation resolution,2,multiple languages,2,biomedical texts,2,word analogy tasks,2,compromising,2,found in,2
a text,2,a rule-based approach,2,compositionality,2,monolingual data,2,the detection,2,adapted to improve,2,ec2 be impr,2
sensitive data,2,chatgpt,2,the effect,2,parameters,2,the best results,2,trained to identify,2,tuned,2
generative models,2,a situation model,2,rule-based approaches,2,comparable accuracy,2,signs,2,exhibit,2,related to,2
calm resource,2,larger parameter sizes,2,transcription,2,sub,2,law,2,compared using,2,adapting,2
a distributional approach,2,a system,2,large-scale datasets,2,the combination,2,time,2,developed to convert,2,integrated into,2
the variability,2,pre-trained word embeddings,2,quality estimation,2,translation accuracy,2,comparable performance,2,generated using,2,known,2
an algorithm,2,the overall performance,2,the elc-bert architecture,2,topics,2,a real-world setting,2,pruning,2,addressing,2
xlm-roberta,2,proof nets,2,nba players' deviations,2,natural language,2,frames,2,addressing,2,avoiding,2
contextual embeddings,2,ocr output,2,wikidata,2,the fluency,2,ter and bleu scores,2,to understand,2,writing,2
a transformer-based model,2,african languages,2,the extraction,2,other types,2,a traditional rule-based approach,2,multimodal,2,distinguish,2
the mondrian conformal predictor,2,mbart,2,the relationships,2,the learning,2,bulgarian,2,implemented using,2,determines,2
the digitization,2,embedding model,2,parallel corpora,2,the time,2,semantic information,2,distinguish,2,make,2
the proposed annotation guidelines,2,a neural model,2,elmo,2,social justice,2,transformer-based models,2,grounding,2,required to achieve,2
sentence,2,sentence alignment,2,relations,2,the polish language,2,english-chinese translations,2,consisting of,2,to support,2
a neural language model,2,adpositions,2,text classification models,2,antnlp,2,the learning,2,filtering out,2,del traine,2
patterns,2,translation accuracy,2,xlm-r,2,these differences,2,different types,2,reduced,2,involving,2
deep neural networks,2,a deep learning model,2,improved performance,2,mt5,2,african languages,2,target,2,model trai,2
the proposed multilingual corpus,2,mandarin chinese,2,the processing,2,relation extraction,2,text classification,2,used to augment,2,recall,2
self-distillation,2,access,2,paragraph-level translations,2,linguistic resources,2,relations,2,utilized to improve,2,developed to improve,2
a bert-based model,2,the fracas test suite,2,additional features,2,human evaluations,2,challenges,2,generalized to handle,2,learned using,2
a neural machine translation system,2,limited data,2,under-resourced languages,2,italian,2,such models,2,decoding in,2,yields,2
algorithm,2,consistent results,2,corpora,2,the robustness,2,annotation,2,to reduce,2,representing,2
the translation quality,2,text classification,2,distant supervision,2,the degree,2,text classification tasks,2,exist,2,hat contribut,2
a neural network,2,indirect supervision,2,the model,2,game,2,cases,2,following,2,collecting,2
a constraint-driven iterative algorithm,2,quality estimation,2,summarization,2,wikipedia,2,a sentence,2,optimized for,2,comparing,2
hybrid grammars,2,sentence length,2,semantic relations,2,the english,2,this finding,2,compare in,2,describing,2
a more efficient method,2,post,2,translation suggestion systems,2,the russian-to-chinese task,2,electronic health records,2,ec1 trained using,2,ec5 compa,2
machine translation metrics,2,user modification capabilities,2,active learning,2,labels,2,the nuances,2,identified as,2,to train,2
an approach,2,long document classification,2,the addition,2,multilingual parsing,2,french,2,to analyze,2,n trained,2
sequence labeling,2,multiple languages,2,linguistic data,2,implications,2,human annotation,2,used to mitigate,2,when compa,2
higher accuracy,2,pre-trained multilingual models,2,idioms,2,data,2,utterances,2,inspired by,2,introducing,2
a supervised learning,2,the structure,2,bpe,2,backtranslation,2,sign language recognition,2,generated from,2,trained to detect,2
pre,2,a large corpus,2,downstream tasks,2,the biomedical domain,2,implications,2,to generalize,2,promote,2
techniques,2,a bert model,2,efficiency,2,text classification,2,distillation,2,ensure,2,,
udpipe,2,downstream applications,2,the precision,2,emotions,2,biomedical abstracts,2,make,2,,
a neural network-based approach,2,the usability,2,the target side,2,deep learning models,2,the effects,2,handling out,2,,
representations,2,atdt,2,a machine reading comprehension task,2,a text,2,khasi,2,adapted to accommodate,2,,
a curriculum,2,slavic languages,2,inference,2,specific features,2,nuances,2,ec1 incorporating,2,,
autoextend,2,fluency,2,natural language processing,2,diversity,2,evaluation metric,2,spoken,2,,
a deep learning architecture,2,end,2,assamese,2,other methods,2,lemmatization,2,to handle,2,,
post,2,frames,2,pre-trained models,2,the translation accuracy,2,these features,2,targeting,2,,
specialized terms,2,syntactic features,2,the system,2,efficiency,2,a precision,2,designed using,2,,
entities,2,the lexicon,2,human judgments,2,annotation projection,2,natural language processing,2,help,2,,
a linear classifier,2,a small amount,2,complex questions,2,the system-level,2,the integration,2,increased,2,,
an automatic system,2,aspect-based sentiment analysis,2,emotions,2,the proposed approach,2,image,2,token,2,,
conjunction,2,transformer-based models,2,sexist content,2,the-shelf,2,recurrent neural networks,2,integrate,2,,
,,a high accuracy,2,understanding,2,the key differences,2,a machine learning model,2,prompting,2,,
,,deep learning methods,2,writers,2,low-resource scenarios,2,,,approaches,2,,
,,novel variants,2,traditional methods,2,participants,2,,,grained,2,,
,,a feature extractor,2,quality,2,empathy,2,,,designed to improve,2,,
,,language modeling,2,the relevance,2,natural language processing techniques,2,,,surpass,2,,
,,speech recognition systems,2,fake reviews detection,2,the ability,2,,,leverages,2,,
,,entity recognition,2,chinese text,2,explanations,2,,,used to represent,2,,
,,the proposed dataset,2,critical error detection,2,user data,2,,,to optimize,2,,
,,domain-specific knowledge,2,covid-19 misinformation,2,the meaning,2,,,improved using,2,,
,,this study,2,processing time,2,unstructured text,2,,,measured in,2,,
,,higher bleu scores,2,word meanings,2,conjunction,2,,,initializing,2,,
,,domain adaptation,2,jhubc,2,a rule-based approach,2,,,established,2,,
,,johns hopkins university bible corpus,2,a recurrent neural network,2,the time complexity,2,,,ec1 to extract,2,,
,,the degree,2,house,2,translation,2,,,used to induce,2,,
,,a noising module,2,argument,2,these techniques,2,,,determining,2,,
,,mtsi-bert,2,social media text,2,a benchmark dataset,2,,,transfer,2,,
,,criteria,2,neural parsers,2,historical documents,2,,,support,2,,
,,metaphors,2,non,2,the comparison,2,,,to accommodate,2,,
,,conventions,2,nlp,2,machine learning techniques,2,,,optimized to reduce,2,,
,,machine learning-based approaches,2,embeddings,2,classification,2,,,reranking,2,,
,,a distributional thesaurus,2,classification,2,models,2,,,convey,2,,
,,wikipedia articles,2,machine translation output,2,human annotators,2,,,to represent,2,,
,,training data,2,clinical text,2,english translations,2,,,injecting,2,,
,,new languages,2,deep learning techniques,2,entity,2,,,required,2,,
,,telegram posts,2,the complexity,2,nlp models,2,,,indicated by,2,,
,,human evaluators,2,codenames,2,genes,2,,,ec1 from,2,,
,,translation,2,hfst,2,the text,2,,,achieved by,2,,
,,a dataset,2,event information,2,das,2,,,perform,2,,
,,wordnet,2,roberta,2,a sentence,2,,,spotting,2,,
,,idioms,2,individuals,2,these methods,2,,,generalized to,2,,
,,attention mechanisms,2,russian,2,users,2,,,used to derive,2,,
,,open information extraction,2,machine learning,2,the lexc formalism,2,,,extended,2,,
,,cognates,2,metaphor detection,2,online news articles,2,,,boosted,2,,
,,shinra-5lds,2,speed,2,german,2,,,extracted,2,,
,,the linguistic patterns,2,labeled data,2,lexical simplification,2,,,tuned to capture,2,,
,,the helsinki finite-state transducer toolkit,2,information extraction,2,identification,2,,,used to classify,2,,
,,addition,2,a high degree,2,xlnet,2,,,encoding,2,,
,,a diverse set,2,translation quality,2,named entity recognition,2,,,feature,2,,
,,finite-state covering grammars,2,high precision,2,the overall accuracy,2,,,required to achieve,2,,
,,news articles,2,offensive language,2,high-resource languages,2,,,collecting,2,,
,,machine learning,2,news headlines,2,a precision,2,,,prioritize,2,,
,,a knowledge base,2,,,,,,,covers,2,,
,,incorrect predictions,2,,,,,,,preprocessing,2,,
,,other language pairs,2,,,,,,,learned from,2,,
,,attention mechanism,2,,,,,,,crowdsourcing,2,,
,,low-resource language pairs,2,,,,,,,to address,2,,
,,a corpus,2,,,,,,,labeled,2,,
,,constraints,2,,,,,,,cluster,2,,
,,the sentiment,2,,,,,,,adapted to perform,2,,
,,sentiment,2,,,,,,,word,2,,
,,,,,,,,,,to incorporate,2,,
,,,,,,,,,,describing,2,,
,,,,,,,,,,differ,2,,
,,,,,,,,,,ec1 to annotate,2,,
,,,,,,,,,,combines,2,,
,,,,,,,,,,annotated,2,,
,,,,,,,,,,used to estimate,2,,
,,,,,,,,,,domain,2,,
,,,,,,,,,,trained to generate,2,,
,,,,,,,,,,tuned to improve,2,,
,,,,,,,,,,distributed,2,,
,,,,,,,,,,affecting,2,,
,,,,,,,,,,translating from,2,,
,,,,,,,,,,balancing,2,,
,,,,,,,,,,sacrificing,2,,
,,,,,,,,,,optimized to improve,2,,
,,,,,,,,,,converting,2,,
